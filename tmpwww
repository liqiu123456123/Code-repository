import torch
import torch.multiprocessing as mp
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# 模型名称或路径
model_name_or_path = "gpt2"

# 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)
model = GPT2LMHeadModel.from_pretrained(model_name_or_path)
model.to('cuda')
model.eval()

def model_inference(input_text, queue):
    # 将输入文本编码为模型输入格式
    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')
    
    # 生成文本
    with torch.no_grad():
        outputs = model.generate(input_ids, max_length=50)
    
    # 解码输出
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    queue.put(generated_text)

def main(input_texts):
    # 创建一个进程间通信队列
    queue = mp.Queue()
    
    # 创建进程列表
    processes = []
    for input_text in input_texts:
        p = mp.Process(target=model_inference, args=(input_text, queue))
        p.start()
        processes.append(p)
    
    # 等待所有进程结束
    for p in processes:
        p.join()
    
    # 从队列中获取所有结果
    results = [queue.get() for _ in input_texts]
    return results

if __name__ == "__main__":
    # 输入文本列表
    input_texts = [
        "Once upon a time",
        "In a galaxy far far away",
        "To be or not to be",
        "A long time ago in a land far far away",
    ]
    
    # 运行主函数
    results = main(input_texts)
    
    # 打印结果
    for result in results:
        print(result)
