3. Variant 3：专注于分解Transformer块中的多头注意力（multi-head attention）。这个变体首先在空间维度上计算自注意力，然后是时间维度，从而每个Transformer块都能捕捉到时空信息。
4. Variant 4：多头注意力（MHA）被分解为两个组件，每个组件使用一半的注意力头。不同的组件分别处理空间和时间维度的token。两个不同的注意力操作计算完成后，时间维度的token被重塑并添加到空间维度的token中，然后作为Transformer块下一个模块的输入。

 作者探索了两种视频片段嵌入方法：均匀帧补丁嵌入（uniform frame patch embedding）和压缩帧补丁嵌入（compression frame patch embedding）。
• 均匀帧补丁嵌入是将每个视频帧单独嵌入到token中，类似于ViT（Vision Transformer）的方法。
• 考虑捕获时间信息，然后将 ViT patch 嵌入方法从 2D 扩展到 3D，随后沿着时间维度提取，通过按一定步长提取时间序列中的“管状”结构，然后映射到token。


1. Video sampling interval：不同的采样间隔在训练初期对性能有显著影响，但随着训练的进行，这些影响逐渐减小。
2. Temporal positional embedding：绝对位置编码在某些情况下能提供稍微更好的结果。
3. ImageNet pretraining： 使用在ImageNet上预训练的模型作为初始权重可以帮助视频生成模型更快地学习，但随着训练的进行，模型可能会遇到适应特定视频数据集分布的挑战。这可能导致性能在达到一定水平后趋于稳定，不再显著提高。
4. Model variants：Variant 1在迭代增加时表现最佳，而Variant 4由于计算效率较高，尽管性能稍逊，但在资源受限的情况下可能是一个不错的选择。
5. Video clip patch embedding：均匀帧补丁嵌入在某些情况下表现更好，因为它可能更好地保留了视频的时空信息。
6. Timestep-class information injection：S-AdaLN方法更有效地将信息传递给模型，从而提高了性能。

11111111111
首先使用 VAE 压缩视频，提取视频 patch 组成 tokens，然后进行扩散-去噪过程，区别在于使用 Transformer 替换 U-Net 预测噪声的均值和方差，并且噪声是4维的，最后解码成视频。 


1111111
Datasets：作者选取了FaceForensics, SkyTimelapse, UCF101和Taichi-HD四个数据集，对于这些数据都同意抽取16-frame video clips，并且分辨率都resize成256x256进行训练
Evaluation metrics：这里采用了三种定量评估治疗，FVD，FID，IS

这里重点关注FVD，因为它是基于图像的FID计算而来，更加符合人类的主观判断
IS是对生成图片清晰度和多样性的衡量，IS值越大越好。作者只在UCF101上评估的IS
FID是反应生成图片和真实图片的距离，数据越小越好**。**专业来说，FID是衡量两个多元正态分布的距离
FVD是一种基于 FID 的视频质量评估指标。与图像级别方法不同，图像级别方法使用 Inception 网络从单帧图像中提取特征，FVD 利用在 Kinetics 上预训练的 Inflated-3D Convnets (I3D) 从视频片段中提取特征。随后，通过均值和协方差矩阵的组合来计算 FVD 分数。
11111111
